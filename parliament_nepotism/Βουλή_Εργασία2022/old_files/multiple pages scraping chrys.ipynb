{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab11b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#set ups\n",
    "#pandas originally cut the data frames, this is used to desplay the full df\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns',None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.hellenicparliament.gr/Vouli-ton-Ellinon/To-Politevma/Ekloges/Eklogika-apotelesmata-New/'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0033c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "\n",
    "for p in soup.find_all('p'):\n",
    "    for link in p.find_all('a',{'target':'_blank'}):#το target εννοεί το attribute που εχει το href, dhladh to blank\n",
    "        \n",
    "        if 'Σύνθεση' in link.text:\n",
    "#             print(link.text)\n",
    "            url = link.get('href')\n",
    "\n",
    "            print(url)\n",
    "            urls.append(url)\n",
    "        \n",
    "        #το href είναι για να πάρω το καθαρό link\n",
    "        print(link.get('href'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_urls = []\n",
    "\n",
    "for p in soup.find_all('p'):\n",
    "    if 'Σύνθεση' in p.text:\n",
    "        url = p.find('a',href=True).get('href')\n",
    "        print(url)\n",
    "\n",
    "        more_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ea9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(more_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd06864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the urls you need in a temporary df\n",
    "#turn part of your more_urls list into a df\n",
    "\n",
    "temp_df = pd.DataFrame(more_urls[1:])\n",
    "\n",
    "#rename your first column\n",
    "temp_df = temp_df.rename(columns={temp_df.columns[0]:'url'})\n",
    "\n",
    "#check out your df\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c28f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://www.hellenicparliament.gr/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new column named final_url\n",
    "temp_df['final_url'] = ''\n",
    "\n",
    "#iterrate over the rows in your df\n",
    "for i, item in temp_df.iterrows():\n",
    "    #if 'hellenicparliament.gr' is already part of your url\n",
    "    if 'hellenicparliament.gr/' in item['url']: \n",
    "        #then you are all set, so your final_url will be equal to the one you already have\n",
    "        temp_df.at[i, 'final_url'] = item['url']\n",
    "    #otherwise\n",
    "    else:\n",
    "        #we'll construct your final_url by adding your current url to the base_url\n",
    "        temp_df.at[i, 'final_url'] = base_url+item['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f360911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = list(range(1,31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdfa3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0628c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in numbers:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9e72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to navigate in this table, you normally click on different pages numbers in the pagination list at the bottom of the table\n",
    "#each time you click on a different table page number, your url changes! But it changes in a predictable way \n",
    "#so, we can construct all the different urls you need to visit in order to navigate the whole list of the MPs\n",
    "\n",
    "#let's do this for the first 9 rows of your temp_df\n",
    "#meaning let's do this for the last 8 parliamentary periods which go back to 1996\n",
    "\n",
    "webpages = []\n",
    "\n",
    "for url in temp_df.final_url[0:9]:\n",
    "    if 'SInthesi-Proigoumenon-Periodon' in url:\n",
    "        for n in numbers[0:15]:\n",
    "            webpage = url+'&pageNo='+str(n)\n",
    "            webpages.append(webpage)\n",
    "\n",
    "    else:\n",
    "        for n in numbers[0:30]:\n",
    "            webpage = url+'?pageNo='+str(n)\n",
    "\n",
    "            webpages.append(webpage)\n",
    "        \n",
    "webpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb2398",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#turn this full list of the webpages you wish to visit into a df\n",
    "df1 = pd.DataFrame(webpages)\n",
    "df1 = df1.rename(columns={df1.columns[0]:'url'})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28587629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ed1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's scrape the names of the MPs elected in the last 10 election rounds\n",
    "#do it all at once\n",
    "\n",
    "#create an empty list\n",
    "names = []\n",
    "\n",
    "#loop through your urls\n",
    "#for each one of those\n",
    "for url in df1.url:\n",
    "    #make a request and visit it\n",
    "    response = requests.get(url)\n",
    "    #make python your script sleep for 3 seconds before making the next request\n",
    "    time.sleep(3)\n",
    "    #parse each one of the pages\n",
    "    soup_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "#     try:\n",
    "    #grab all the rows found in the table on each one of the webpages you visit\n",
    "    rows = soup_doc.find('table').find('tbody').find_all('tr')\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "    #grap all the cell values for each one of the rows you got but for the last row in each table (with is the pagination bar)\n",
    "    for row in rows[:-1]:\n",
    "        cells  = row.find_all('td')\n",
    "\n",
    "        if 'SInthesi-Proigoumenon-Periodon' in url:\n",
    "            for cell in cells[0::3]:\n",
    "                print(cell.text.strip())\n",
    "                names.append(cell.text.strip())\n",
    "                \n",
    "        else:\n",
    "#           loop through your cells that hold MP's names\n",
    "            for cell in cells[1::3]:\n",
    "#               print the names as you scrape them\n",
    "                print(cell.text)\n",
    "#               append your names list with the cell values (names) you are getting\n",
    "                names.append(cell.text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab2e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "9*300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbaf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.names.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
